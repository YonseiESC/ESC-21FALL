## ğŸ’» Programming HW description ğŸ’»

ì‹œì‘í•˜ê¸° ì „ì— `hw1.Rmd` íŒŒì¼ê³¼ `home1-part1-data.R`, `home1-part2.data.R` ë°ì´í„°ë¥¼ ë‹¤ìš´ë°›ì•„ì„œ ê°™ì€ working directoryì— ìœ„ì¹˜ì‹œì¼œì£¼ì„¸ìš”.

ì´ descriptionê³¼ Rmd íŒŒì¼ì„ í•¨ê»˜ ì½ì–´ê°€ë©´ì„œ `# TODO` ë¼ê³  ëœ ë¶€ë¶„ì„ ì±„ì›Œì„œ ë…¸íŠ¸ë¶ì„ ì™„ì„±í•´ë´…ì‹œë‹¤.

(âœï¸reference : University of Washington STAT435 Intro to Statistical Learning HW1)

ğŸ“ ì–¼í• ë³´ë©´ ë¬¸ì œê°€ êµ‰ì¥íˆ ë§ì•„ë³´ì´ì§€ë§Œ ë§‰ìƒ `#TODO` ë¼ê³  ë˜ì–´ìˆëŠ” ë¶€ë¶„ì€ ê·¸ë¦¬ ë§ì§€ ì•ŠìŠµë‹ˆë‹¤! ê²ë¨¹ì§€ ë§ˆì„¸ìš”:)

ğŸ“ì œê°€ í–ˆë˜ ìˆ™ì œ(ë§Œì  ëª»ë°›ìŒ..)ë¥¼ ë³€í˜•í•œ ê²ƒì´ê¸° ë•Œë¬¸ì— ì–¼ë§ˆë“ ì§€ ë” ë‚˜ì€ ë°©ë²•ì´ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. Rmd íŒŒì¼ê³¼ ì´ descriptionì€ ê°€ì´ë“œë¼ì¸ì´ê³  ë³¸ì¸ì´ ë” íš¨ìœ¨ì ì¸ ë°©ì‹ì´ë‚˜ ë‹¤ë¥¸ ë°©ì‹ìœ¼ë¡œ ì§œê³  ì‹¶ë‹¤ë©´ ì–¼ë§ˆë“ ì§€ ê°€ëŠ¥í•©ë‹ˆë‹¤!

ğŸ“ì˜ˆì‹œë‹µì•ˆì€ ìˆ˜ìš”ì¼ì— ì—…ë¡œë“œí•˜ê² ìŠµë‹ˆë‹¤!

### Part 1. Implement and experience KDE on your own!

ë¨¼ì € ê°„ë‹¨íˆ ì‹œë®¬ë ˆì´ì…˜ëœ ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤. sin(x) í•¨ìˆ˜ì— Gaussian noiseë¥¼ ë”í•œ ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

**(a)** Rì— ë‚´ì¥ëœ í•¨ìˆ˜ì¸ `ksmooth` ê°€ ì–´ë–»ê²Œ ì‘ë™í•˜ëŠ”ì§€ ë´…ì‹œë‹¤! Gaussian kernelì„ ì‚¬ìš©í•  ë•Œì™€ Uniform kernel (box kernel)ì„ ì‚¬ìš©í•  ë•Œ ì–´ë–»ê²Œ ë‹¤ë¥¼ê¹Œìš”? ê²½ìš°ì— ë”°ë¼ bandwidthë¥¼ ì¤„ì´ë©´ uniform kernelê°€ ëŠê¸°ëŠ” í˜„ìƒë„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

**(b)** ì´ì   ì§ì ‘ kernel smoothingí•˜ëŠ” í•¨ìˆ˜ë¥¼ ë‹¤ìŒê³¼ ê°™ì´ êµ¬í˜„í•´ë´…ì‹œë‹¤. 

`ksmooth.train(x.train, y.train, bandwidth=0.5)` with output `x.train`, `yhat.train` (`x.train` ì€ ì •ë ¬ë˜ì§€ ì•Šì•˜ë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤.)

ì„¸ì…˜ë•Œ ë°°ì› ë˜ Nadaraya-Watson averageì™€ Gaussian kernelì„ ì´ìš©í•˜ë©´ ë©ë‹ˆë‹¤.

ì¤‘ì‹¬ì„ ê¸°ì¤€ìœ¼ë¡œ quartile (75%)ì— í•´ë‹¹í•˜ëŠ” densityê°€ +/- 0.25*bandwidth ë‚´ì— ë“¤ì–´ì˜¤ë„ë¡ scaling í•´ì¤ë‹ˆë‹¤. (ì´ê±´ ë˜ì–´ìˆìŒ)

**(c)** (a)ì—ì„œ `ksmooth` ë¥¼ ì´ìš©í•´ì„œ í”¼íŒ…í–ˆë˜ $\hat{f}$ ê³¼ ì—¬ëŸ¬ë¶„ì˜ `ksmooth.train` ì„ ì´ìš©í•´ í”¼íŒ…í•œ ê²ƒì´ ë¹„ìŠ·í•œì§€ ì‹œê°í™”í•´ì„œ ë¹„êµí•´ë´…ì‹œë‹¤!

**(d)** ISLRì˜ Wage datasetì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤. (`home1-part1-data.R`)

(ì›ë˜ëŠ” `ksmooth.predict` ë„ êµ¬í˜„í•˜ì—¬ test setì—ì„œë„ evaluationì„ í•´ì•¼ í•˜ì§€ë§Œ ì´ ê³¼ì •ì—ì„œ interpolationì„ í•´ì•¼ í•˜ëŠ” ë“± ë²ˆê±°ë¡œì›Œì§€ê¸° ë•Œë¬¸ì— ì´ë²ˆì—” train dataë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤!)

ë°ì´í„°ì˜ scatterplotì„ ê·¸ë ¤ë³´ê³  êµ¬í˜„í•œ í•¨ìˆ˜ë¡œ í”¼íŒ…í–ˆì„ ë•Œ ì–´ë–»ê²Œ í•¨ìˆ˜ê°€ ì¶”ì •ë˜ëŠ”ì§€ í™•ì¸í•´ë´…ì‹œë‹¤.

ê·¸ëŸ° ë’¤ bandwidthê°€ 1, ..., 10ê¹Œì§€ë¡œ ëŠ˜ì–´ë‚¨ì— ë”°ë¼ training errorëŠ” ì–´ë–»ê²Œ ë³€í•˜ëŠ”ì§€ ë´…ì‹œë‹¤.



### Part 2. Optimal Bandwidth

ì´ë²ˆì—ëŠ” bandwidthì— ë”°ë¥¸ bias-variance tradeoffë¥¼ ë´…ì‹œë‹¤.

Training sample $(x_1, y_1), ..., (x_n, y_n)$.

Predictor values $x_1, ..., x_n$ are considered fixed (not random).

Response values $y_i=f(x_i)+\epsilon_i$, with $\epsilon_1, ..., \epsilon_n$ independent, $E(\epsilon_i)=0, Var(\epsilon_i)=\sigma^2$

Define $\textbf{y}=(y_1, ..., y_n), \textbf{f}=(f(x_1), ..., f(x_n))$, etc.

$K_\lambda(x)$ : Kernel with bandwidth parameter $\lambda$ controlling the amount of smoothing.

Define $\textbf{W}$ : n x n weight matrix with $\textbf{W}_{i,j}=\dfrac{K_\lambda(x_i-x_j)}  {\sum_j K_\lambda(x_i=x_j)}$ 

Then $\hat{f}=Wy$.

**(a)** ìœ„ì˜ specificationì— ë”°ë¼ì„œ Wë¥¼ ë§Œë“¤ê³  (Part1ì—ì„œì™€ ê±°ì˜ ë™ì¼í•©ë‹ˆë‹¤!) `sigma=seq(from=0.01, to=2, by=0.01)` ì— ë”°ë¼ì„œ variance, bias^2, sum(MSE)ë¥¼ ì €ì¥í•©ë‹ˆë‹¤. 

**(b)** (a)ì˜ ê²°ê³¼ë¥¼ ë³´ê³  optimalí•œ lambdaê°’ (ì´ ê²½ìš°ì—” $\sigma$)ì„ ì°¾ì•„ì„œ $\hat{f}$ ë¥¼ êµ¬í•˜ê³  ì£¼ì–´ì§„ $f$ì™€ ë¹„êµí•´ë´…ì‹œë‹¤.

**(c)** simulated dataì— ëŒ€í•´ì„œ bandwidthì— ë”°ë¼ fitted lineì´ ì–´ë–»ê²Œ ë³€í™”í•˜ëŠ”ì§€ ê´€ì°°í•˜ê³  (a)ì˜ ê²°ê³¼ì™€ ë¹„êµí•´ì„œ ì„¤ëª…í•´ë³´ì„¸ìš”!